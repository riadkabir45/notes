- [Unveiling the Past: A Holistic Approach to Rescuing Historical Texts through Advanced Image Analysis and AI](https://drive.google.com/file/d/1k5mKpEci6vCH1c3-qj0tj-qlSdS_8qHy/view?usp=sharing)#Thesis
	- Abstract
		- In recent years, more and more administrative and legal paperwork has been done using computers because it's efficient. But in the past, when technology wasn't as good, there were problems with printing documents. This meant that text could be hard to read, the ink might not be consistent, and documents could get damaged over time. This caused a lot of important information, especially in ancient scripts, to be lost because it couldn't be digitized. Our research is all about solving this problem. We want to find a way to automatically get accurate information from pictures of these old documents. We employ a multi-step approach to enhance text recovery. Firstly, we utilize adaptive Gaussian threshold to improve image clarity by removing excess ink or stains. Next, we apply optical character recognition (OCR) using advanced systems like Easy OCR and Tesseract, followed by thorough database validation for result accuracy. But sometimes, even with the special software, we can't see all the words because they're too faded or missing. To overcome this, we employ Natural Language Processing (NLP) techniques with Happy Transformer, a tool based on Hugging Face's Transformer Library. This enables us to predict and reconstruct missing letters or words. After prediction and cross-referencing with the database, the identified words are seamlessly integrated into the text, ensuring data retrieval. Our research is important for preserving historical and legal documents that might otherwise be lost forever.
	- Problem
		- Old documents are very hard to integrate in the efficient and convenient digital space of modern era. Key challenges in digitization includes aging, fold stains and blurriness.
	- Solution
		- Use of 3 layered system to digitize damaged documents that includes pre-processing as first layer. Easy OCR as second layer and a NLP model as third layer
	- Related Work
		- Researches on Turkish documents using up-sampling and threshold techniques like Otsu or triangle or adaptive method to achieve 18% accuracy boost
		- A Raspberry Pi based OCR system to aid visually impaired individuals using hidden Markov model and text to speech conversion
		- Utilize brightness and chromaticity to pre-process image for significant OCR accuracy
	- Methods
		- Pre-Processing
			- Gray Scaling
			- Threshold
				- Adaptive Threshold (Best performance)
				- Simple Threshold
				- Binary Threshold
				- Otsu's Threshold
				- Truncated Threshold
				- Zero Threshold
		- OCR
			- Easy OCR(Best for more features to improve accuracy)
			- PyTessarect
		- Post OCR Correction
			- Grammatical and Contextual Analysis
			- Mark misspelled or out of place words or symbols
			- Breaking into parts for correction performance
			- NLP
				- Using Google BERT model to predict marked words or symbols
				- Using ULMFiT
		- Metrics
			- SuperGLUE benchmark
			- Recall, Precision, F1, Fbeta
		- Results
			- BERT reached optimum accuracy with maximum dataset range in minimum epoch and is not affected by higher runs
			- ULMFiT fluctuates on few epoch and performs better on higher runs
		- Future scopes
			- Deep models
			- Other languages
- [Enhancing OCR in historical documents with complex layouts through machine learning](https://drive.google.com/file/d/19iwscheebl0Yl-XvF_CQGViXg_p1HmMx/view)
	- Abstract
		- This paper explores the challenge of processing and extracting information from large quantities of printed serial sources from the 19th century, which have been largely untapped due to the inadequacies of existing extraction techniques. We focus on the Habsburg Central Europe’s Hof- und Staatsschematismus, a comprehensive record published between 1702 and 1918 that documents the Habsburg civil service’s hierarchy and the evolution of its central administration over two centuries. Our approach sees the significant investment into machine learning-driven layout detection prior to the OCR-process. We generated synthetic data mimicking the Hof- und Staatsschematismus style for initial training of a Faster R-CNN model, followed by fine-tuning the model with a smaller dataset of manually annotated historical documents. Subsequently, we optimised Tesseract-OCR for our document style to enhance the combined structure extraction and OCR process. Our evaluation demonstrates significant improvements in OCR performance metrics (WER and CER), with the combined structure detection and fine-tuned OCR process showing a decrease in error rates of 15.68 percentage points for CER and 19.95 percentage points for WER. These findings underscore the potential of ML techniques in facilitating the extraction and analysis of historical documents.
	- Problem
		- Most of the history of earlier eras are written in physical documents. With these data, one could achieve far greater understanding of those era. But extracting data from enormous papers are tedious. Though ORC technologies have been invented for these kind of works, complex documents can easily hamper the performance of these tradition ORC systems.
	- Solution
		- The goal is to process documents by breaking them into parts by their layout. Which can simplify them enough for OCR to process them with greater accuracy
	- Previous works
		- There have been many works on processing historical documents, but each works better on different category documents using different kind of tools and most of them fail to work on larger documents such as the Schematismus
	- Dataset
		- PDFs of the Hof- und Staatshandbücher from Austrian National
		  Library, Munich Digitalization Center or archive.org
		- Names from Austria, Hungary, Switzerland, and Germany
		- Headlines from Austrian orders and decorations
	- Methodologies
		- Font Preparation
			- Use Font Forge to customize closely matched font Opera-Lyrics-Smooth to get exact font
		- Dataset generation
			- Using python script to generate Latex code
			- Compile Latex to PDF
			- Package zref-savepos was used to store box coordinates
		- Layout detection
			- Faster R-CNN built on ResNet-50 using PyTourch
				- Pre-training improved training performance
				- Tweaked anchor generation
				- Max object limit set to 1000 to avoid data loss
				- Up scaling the input size of model
				- Batch size to 2
				- Each epoch goes under validation requirements
			- Issues
				- Bound box overlapping
					- Marge box by IoU
				- Box clips characters
					- Box padding
		- Extraction
			- Training Tessarect with Custom font generated image using custom character map
		- Evaluation
			- 18 original documents hand annotated
				- Accuracy: 0.968
				- Precision: 0.929
				- Recall: 0.925
				- F1: 0.919
				- Bound Box Accuracy: 0.909
			- Performance difference
				- Straight forward: CER 20.01 WER 41.05
				- Font fine tuned: CER 13.00 WER 30.11
				- Layout detection: CER 5.13 WER 23.73
			-