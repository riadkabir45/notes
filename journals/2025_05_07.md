- Thesis Resources #Thesis
	- [Registration Page and Deadlines](https://docs.google.com/spreadsheets/d/1SkyO77YFde-N4bpxQ5lb07FH24hFINyR4JBgD47Xamk/edit?gid=562208551#gid=562208551)
	- [Rubrics](https://docs.google.com/spreadsheets/d/1PnvmE6MaldPadO8S5bjHajgxZTfdZEz4/edit?gid=384748274#gid=384748274)
- [Optical character recognition system with natural language processing for data recovery on scanned old academic card reports](https://drive.google.com/file/d/1myNaubL6TxHEQ3zodfi2q-e028346CXZ/view?usp=sharing) #Thesis
	- Abstract
		- In the digital age, preserving and effectively retrieving historical academic records has a significant challenge, especially when these documents only exist in deteriorated physical formats. We propose an approach to recover data from scanned documents of grade records, by using image processing and Natural Language Processing (NLP) to enhance the accuracy of Optical Character Recognition (OCR) in these documents, essential for the preservation of digital records. Our three-step methodology: first, improves the quality of the scanned image; then, extracts text using OCR and NLP techniques to retrieve data from old physical grade cards; and finally, the extracted data is corrected using Chat-GPT and prepared for upload. The results are assuring, showing an impressive Character Error Rate (CER) of 2.15% and a Word Error Rate (WER) of 7.05%, demonstrating the high accuracy of the OCR system used and its ability to precisely extract text from scanned documents. These low error rates achieved, as a result to the successful implementation of pre-processing and post-processing techniques, as well as the use of an advanced OCR tool, underscore the potential of this OCR approach to effectively extract information from documents.
	- Problem
		- Physical documents are prone to make task difficult, easily deteriorate and are inefficient.
		  digitizing them can mitigate this issues. But old documents are often very hard to digitize as it is very dull work for human and damage documents are very hard for AI models to digitize
	- Solution
		- Digitize old documents using Image Processing and Machine Learning or Neural Network and correct damaged portions via Large Language Model contextual correction such as ChatGPT.
	- Methodologies
		- Pre Processing
			- Conversion: More supported format
			- Gray Scaling: Remove color noises
			- Image Threshold: Removing anything except between 10-199
			- Split Data: Different processing for Tables and Meta data
			- De-speckle: Remove speacke from images
			- De-skew: Fix image orientation
			- Morphological Enhancements: modify the contour of objects within an image
			- Binarization: Increase image clearity
			- Gaussian Blur: Smoother edges without making them blurry (Kernel 1)
			- Dilatation Enhancements: thickness of the characters (Kernel 20x20)
		- Character Recognition
			- PyTessarect: Extract string segmentation
		- Post Preparation
			- White Space Cleaning: Using python string methods to remove and fix extra white spaces or incorrect punctuation marks
			- Manual intervene: Correction of text that was not possible with automated system
			- ChatGPT: Correction via tokenization
		- Calculation:
			- Error = (Substations + Deletions + Insertions)/N
	- Diffuculties
		- Pre-processing
			- Loss of characters
			- Removing lines from tables: Damaged meta data
		- Character Recognition
			- Failed to recognize names
			- Recognize numbers in tables
		- Results:
			- Table:
				- CER: 10.26%
				- WER: 19%
	- Results:
		- CER: 2.15%
		- WER: 7.05%
		- F-score: 0.78