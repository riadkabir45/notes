- [Invizo: Arabic Handwritten Document Optical Character Recognition Solution](https://arxiv.org/pdf/2502.05277) #Thesis
	- Abstract
		- Converting images of Arabic text into plain text is a widely researched topic in academia and industry. However, recognition of Arabic handwritten and printed text presents
		  difficult challenges due to the complex nature of variations of the Arabic script. This work proposes an end-to-end solution for recognizing Arabic handwritten, printed, and Arabic numbers and presents the data in a structured manner. We reached 81.66% precision, 78.82% Recall, and 79.07% F-measure on a Text Detection task that powers the proposed solution. The proposed recognition model incorporates state-of-the-art CNN based feature extraction, and transformer-based sequence modeling to accommodate variations in handwriting styles, stroke thicknesses, alignments, and noise conditions. The evaluation of the model suggests its strong performances on both printed and handwritten texts, yielding 0.59% CER and & 1.72% WER on printed text, and 7.91% CER and 31.41% WER on handwritten text. The overall proposed solution has proven to be relied on in real life OCR tasks. Equipped with both detection and recognition models as well as other Feature Extraction and Matching helping algorithms. With the general purpose implementation, making the solution valid for any given document or receipt that is Arabic handwritten or printed. Thus, it is practical and useful for any given context.
		- Challenges
			- Cursive style
			- Diverse hand writing
			- Similar character
			- Contextual based character recognition
			- The use of Diacritics
			- Ligature characters
			- Positional styles
			- Right to Left
			- Scarcity of proper dataset
				- Synthesized data with fonts such as Naksh, Rekaa and Diwany
				- Noisy background generation
		- Related works
			- Useful methods
				- Locate character or component and group them to word
					- Probability based ER extraction and filtration based on maximum probability to get reduced set of potential characters
					- Grouping potential character to words
				- Filtering region of interest proposals with random forest classifier. Using two CNN to further filter the proposals and extract data. Merging results accurately using Instance-aware component grouping
				- Breaking the image into smaller parts while keeping original sequence via SegLink
				- Using regression(Rotation Sensitive Regression Detector)
				- Pixel level regression
				- Dimension-decomposition Region Proposal Network
					- Can detect text size very different compared to main text
					- Cannot handle complex documents
				- Sequence based detection using LTSM(Bi-LSTM) and CNN
					- Semantic labeling with FCN and MSER
					- Progressive Scale Expansion Network to handle arbitrary text size
					- Cluster pixel extraction
			- Datasets
				- KHATT
				- Arabic Multi-Fonts (AMFDS)
				- Hijjaa benchmark
				- Arabic Handwritten
				  Character Dataset
			- Key methodologies
				- Structural features with HMM (92% Accuracy)
				- CNN-BLSTM-CTC based sequencing (80.02% Accuracy)
					- Arabic Multi-Fonts dataset (85.15% Accuracy)
				- E2E OCR on Differentiable Binarization and Adaptive Scaling(83.75% Accuracy)
				- Hybrid Network
					- CNN-Bi-GRU (91.78% - 97.05%)
		- Dataset
			- AMFDS
			- ImageNet image classification dataset
			- AraBERT
			- Naqsh Arabi OCR Dataset (Generated)
			- ICDAR 2015
			- Total Text
			- MSRA-TD500
			- Chinese Baidu
			- Arabic 2023
			- KHATT
		- Experiments
			- CNN-BLSTM-CTC
				- CNN as the feature extractor trained on generated randomized dataset
				- Processing via BLSTM
				- Converted to character via CTC
				- Using AMFDS dataset
				- On learing rate 5*10^-5
				- Failed to converge
				- Re trained with 2-3 words  sentence
				- Noticed decline on accuracy on long sentences
			- CNN-MiniLSTM-SelfAttension-CTC
				- Using Mini-LSTM instead of BLSTM to increase training speed and solve longer sentence issue
				- Adding self attention layer for different weighting of each sequence for better dependency understanding
				- Slightly improved accuracy
			- Vision Encoder-Decoder Transformers
				- Using Swin V2 as encoder
				- Encoder wieghts based on ImageNet image classification dataset
				- Decoder weights based on AraBERT
				- Tested on Naqsh
				- Image compression causes loss of data compared to 1024x1024 input size
		- Methodologies
			- Detection
				- Based on DBNet++
				- Fine tuned universal best-weights on ICDAR 2015, Total-Text,
				  MSRA-TD500, and Chinese Baidu
				- Model
					- Extract features via ResNet50 and scale up till fit for Adaptive Scale Fusion(ASF)
					- ASF generates contextual features for probability map and threshold map
					- Apply supervision on each maps
					- Generate bounding boxes
				- Segmentation Dataset
					- Using 2 classes of Arabic 2023 dataset to for line segmentation of hand written text as well as printed texts and numerals
			- OCR
				- Using CNN to extract features
				- Convert features to texts using Encoder Decoder Architectures
			- Dataset
				- KHATT Dataset
					- Split
						- 6974 training samples
						- 767 validation samples
						- 766 test samples
				- Naqsh Arabi OCR Dataset
					- Created using e Abstract Window Toolkit graphics Java package
					- 21 different calligraphy
					- Generated from Wikipedia pages
					- Split
						- 84000 training samples
						- 24000 validation samples
						- 24000 test samples
				- Arabic Numerals Dataset
					- Split
						- 4900 training samples
						- 1050 validation samples
						- 1050 test samples
				- Pre processing datasets
					- Noise reduction
					  Standardized background
					- Enhanced contrast
					- Size normalization
			- Model Architecture
				- Transformer based model
				- CNN model  for feature extraction
					- RGB image of 1024*64 through three layers each with their batch normalization and max pooling
				- Encoders for spatial dependencies
					- Using multi head self attention to model dependencies in sequence
					- Self attention allows every step to attend all parts for extensive relationship modeling far apart
					- Using sinusoidal positional encoding for contextual awareness
						- Effective for longer sequence
				- Decoders sequence generation
					- 6 layered decoder on the output sequence given previous sequence
				- Augmentation
					- Background noise
					- Motion blur and low resolution
					- Rotation and distortion
					- Salt and paper noise and Gaussian noise
			- Main Module
				- Template Data
					- Handle inputs based on predefined formats and channel through required processes
				- Pre processing
					- Gray scale
					- Fast Non Local Means for denoise
					- Binarizing using threshold
					- Morphological transformation
				- Detect region
					- Extract key points using Scale-Invariant Feature Transform
					- Brute force matching Using L2 Norm
					- Homography matrix
				- OCR Processing
					- Check data type for paragraph or line
						- On paragraph, it is segmented
				- Enhance quality
					- Type based output correction
						- Closest Levenshtein distance
		- Results
			- Best weight DBNet vs Fine tuned
				- DBNet at CER 22% WER 56%
				- Proposed model at 7.91% and 31.41%
		- Issues
			- Proper denoising dots
			- Detecing regions properly
			- Poorly detects other writing style