- [TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document](https://doc-00-bc-prod-03-apps-viewer.googleusercontent.com/viewer2/prod-03/pdf/ouahe2sk435vlj9c92j53k7sdadi8djc/hsjtsa9ri9p00j9amcfam60gtkrfprl1/1749397500000/3/111657066452354275804/APznzabnw3BOXQMImPhBI88j4tXIRrq50xSm3evYwOd_ZulxcRMz8EafzU35KvRcgCtVbZTyVskNFRjcgMCUK_KrGDkZAXhzeZp5JFIaQkIsDs04oLTvqq4r8LWs54HvXkb1e__XWGVVD3WVa5cMQv2i1mZJhTu3s2aHBGH8s9p0mBrxf-KzK0vYJecUV6gnMW3gMsjv7tmqFhloC8BnCQM3WtZkTn4qkuhC_jWSIes-mGngKwQchGCY-WiYudpzKOv5LwsVzAsefmLkYD8ULKjLcmE2nFKft46AE7qLkK_44tOyap6VVEDm0kr87OQeuZln0I1__VSeWmjo5C-plwnL91zg0Lm1mMaQrij5ICdJOuYS2sEj39ZDBwZK8_QWC_HH-33abS4gIw_LAlO-A4nYhe_wSc013HiH5scH94dFUiZuS0rv6PRN88cwIQLcNwsKnLe1sLBrZ2atYPr7pUlTVKUMIR_zObjALd-7X55UVb5oMeSDIbrLLYiUiT6V0QdEJuVYu73et08B8TCUug3xCmSPLbX64BS5J1tRoDj6hEL-f-ihTXYujJc7QAZ3nK4VVBZnIefQ7QjpA3Sg_B_1hTTF_AmDddMv6J4O41OT0DWgfgAOIIEs-UhBTmsfCbL3ELp4bACnbEYmyDnZFG18IDIQgwm-ZokKcOeNcG0FdhQLh6hLif832DJbjZ71ZbUrNJ_JAez7ZetSEgIaflg2Q_FpA1wNzC_v6Et5T0J-p4Xu8Oovo1exaW3OvsH1_C6lF-Qeg2KvxyMGxrs-UQH8uGBgCzWkusnNTTtIVQB50-flgma5TLXadcrAnjv9fUw5-Xp3L1KhwwSzmgrAxWP6bcwmPDF9EhNCKL38Zt9mHcVZIIT5evJRgJH90uvCPqm2iY73zwMV3Iy_MOdL63g47rV2j5lgHcxopfOHnCKFLYfcVeCDcUAyC-m5aQRjJlPN02jQCk9S9-uqoo9JIRjQhyERVu4PrzEH8Ms1u9pk8mbf12uy9Hb1FZlY0qdcEhQbNgihwOlQf-uP3sfE3EoBFhzTxlX4F8YJc4_7qfQQIG9PZF-LwcBvikDOnthpE4DKxcZK6AQZonKj8P02_FmEu9f5yfgVVRRM8e08FvMWJYSA55ji9wObBhIMpaD8R1HUc0EAUAiaMoBof5lHhSOshaiv4eJHdi1LomcyVBTE3QkTR7qTjrGU7TiEuxhRB2zD2FXr1LY893rYlHUyYfJdZO0E-N9vOI0Be7WoSemsVq1XTZQo_BSCwGdJJk6pU-euARc0yVxTWbG8TC2I2lZo7UPRdF6sXo09WB0cpTxB5654jrKMNfXjNSJdDMRxs6IvlpOUXt7CXTpW6wvombfHGPJKMrFnC9m3EkbekP5zgXHRzGv4VqFaZZ7Q-E98G2C9fcAoAiR4fxMefL903aMuOM423BfISJPpu6DRTpkvDRKKeFeEBkXNjOL-ZakAbWU5PqlX3pgFD_cAG7zUL8NA9nX4lcu6GxoOwEUsVXqkqoq7zAWwrLGFaQmAzRUTa8CA1RV8n6U9jsFhD4VjMxEPtS0LDxLQoXHb3Hv4XU38VGA7HkGnSE522lnsrmv8blHIeUDNqa3V3Ef9aatqUIRMy-3lDQUZPcxVQuPulDbWmqSvFmhQWVlC6hqEXThFa4uVErGZjDTBalRy7wUEJjueIFvm4N4f0lMxHDHSFp8OM8gShMfZNSoYGcxxVYhqgE0GN88xAmBqJSyBr1aNMFNXiz6ECww0tdl0XcCn1XSrLh-GkJaLFJlCWcKz8OQ9KvOkrBSIJh3DkdZ9htn4XVIIsAXx7thPJgIZ_3bNwv5erY14HiLY44nDUZOwRlyMGYHnTjM3qIudJdIk4J2GpqwpLJf_FolC_88w6dPi9JwLjWQznWNfOAxS1DSZ0SYPJD5mdsgO5IwAFWSEFDxYooceTaqYYUIt9vsZZ4HJN1F9n_JZyAtsbmjqjWDRnPaHvEV_K046fNJiy5VA5un55Z8agje2z78nst1F2zVbo9Mo5_jBD8IkkTH8oHzKIZ1nBfFL9GKRvZdjo2fmV9Qf0EbqyA0Yf4W26Q==?authuser=0&nonce=uliesiot3e5sg&user=111657066452354275804&hash=rmr31j4h8j60a315i3r3g0dfjjiulblb)
	- Abstract
		- We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model’s performance. Moreover, by expanding our model’s capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. It also learns to perform screenshot tasks through finetuning. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, eepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding.
	- Problem
		- Early methods of extracting data from ducuments revolved around two parts. First is extracting text from the docuement. Second i